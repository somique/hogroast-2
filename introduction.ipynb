{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa9e9386",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook will introduce the data available in this project and how to access it.\n",
    "\n",
    "### Data\n",
    "\n",
    "The data totals ~1GB and is accessible from the following [Google Drive link](https://drive.google.com/drive/folders/13_goQHv07qy-fJILSXxOxyiKtzrr5BRe?usp=sharing). You will find two main files in the repo:\n",
    "1. abmelt.pkl\n",
    "2. martini.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f07893",
   "metadata": {},
   "source": [
    "These two files correspond to simulation datasets for all 47 antibodies for the all-atom abmelt protocols and Martini3 coarse grained protocol respectively. Each is a pickle file which is easily accessible by python in a similar format to dictionaries via keys. Let's first look at the abmelt data (all-atom simulations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "578ebdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of antibodies = 47\n",
      "dict_keys(['abituzumab', 'abrilumab', 'adalimumab', 'alirocumab', 'anifrolumab', 'atezolizumab', 'bapineuzumab', 'basiliximab', 'bavituximab', 'benralizumab', 'bevacizumab', 'blosozumab', 'bococizumab', 'brentuximab', 'briakinumab', 'brodalumab', 'canakinumab', 'carlumab', 'certolizumab', 'clazakizumab', 'codrituzumab', 'crenezumab', 'dacetuzumab', 'daclizumab', 'daratumumab', 'denosumab', 'dinutuximab', 'duligotuzumab', 'emibetuzumab', 'enokizumab', 'epratuzumab', 'etrolizumab', 'farletuzumab', 'fasinumab', 'ficlatuzumab', 'fulranumab', 'ixekizumab', 'muromonab', 'obinutuzumab', 'otelixizumab', 'ozanezumab', 'ponezumab', 'rituximab', 'seribantumab', 'simtuzumab', 'vedolizumab', 'veltuzumab'])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "data_location = \"data/data\" # edit this path to wherever you downloaded the data to\n",
    "\n",
    "with open(f'{data_location}/abmelt.pkl', \"rb\") as f:\n",
    "    abmelt_data = pickle.load(f)\n",
    "\n",
    "# we can see that at the highest level data is grouped by antibody\n",
    "print(\"Number of antibodies =\", len(abmelt_data))\n",
    "print(abmelt_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f1b26a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features = 22\n",
      "dict_keys(['angles', 'bonds', 'charge', 'coloumb14', 'coloumbSR', 'contacts', 'covar', 'dipole', 'distances', 'energy', 'enthalpy', 'gyr', 'kinetic', 'lj14', 'ljSR', 'partition-sasa', 'per-block-s2', 'per-res-s2', 'potential', 'rmsd', 'rmsf', 'sasa'])\n",
      "Feature data type = <class 'pandas.core.frame.DataFrame'>\n",
      "All features are pandas dataframe? True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# let's look specifically at the first antibody abituzumab\n",
    "print(\"Number of features =\", len(abmelt_data[\"abituzumab\"]))\n",
    "print(abmelt_data[\"abituzumab\"].keys())\n",
    "print(\"Feature data type =\", type(abmelt_data[\"abituzumab\"][\"angles\"]))\n",
    "print(\"All features are pandas dataframe?\", all(isinstance(abmelt_data[\"abituzumab\"][x], pd.DataFrame) for x in abmelt_data[\"abituzumab\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb1394a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature angles shape = (501, 15)\n",
      "Feature bonds shape = (8001, 6)\n",
      "Feature charge shape = (10, 21)\n",
      "Feature coloumb14 shape = (8001, 3)\n",
      "Feature coloumbSR shape = (8001, 3)\n",
      "Feature contacts shape = (8001, 3)\n",
      "Feature covar shape = (1620, 3)\n",
      "Feature dipole shape = (8001, 12)\n",
      "Feature distances shape = (501, 18)\n",
      "Feature energy shape = (8001, 3)\n",
      "Feature enthalpy shape = (8001, 3)\n",
      "Feature gyr shape = (8001, 96)\n",
      "Feature kinetic shape = (8001, 3)\n",
      "Feature lj14 shape = (8001, 3)\n",
      "Feature ljSR shape = (8001, 3)\n",
      "Feature partition-sasa shape = (8001, 3)\n",
      "Feature per-block-s2 shape = (8, 3)\n",
      "Feature per-res-s2 shape = (215, 3)\n",
      "Feature potential shape = (9, 24)\n",
      "Feature rmsd shape = (8001, 6)\n",
      "Feature rmsf shape = (113, 21)\n",
      "Feature sasa shape = (8001, 21)\n",
      "angles (501, 15)\n",
      "bonds (8001, 6)\n",
      "charge (10, 21)\n",
      "coloumb14 (8001, 3)\n",
      "coloumbSR (8001, 3)\n",
      "contacts (8001, 3)\n",
      "covar (1620, 3)\n",
      "dipole (8001, 12)\n",
      "distances (501, 18)\n",
      "energy (8001, 3)\n",
      "enthalpy (8001, 3)\n",
      "gyr (8001, 96)\n",
      "kinetic (8001, 3)\n",
      "lj14 (8001, 3)\n",
      "ljSR (8001, 3)\n",
      "partition-sasa (8001, 3)\n",
      "per-block-s2 (8, 3)\n",
      "per-res-s2 (215, 3)\n",
      "potential (9, 24)\n",
      "rmsd (8001, 6)\n",
      "rmsf (113, 21)\n",
      "sasa (8001, 21)\n",
      "Total columns for abituzumab per temp: 92\n",
      "Total datapoints for abituzumab: 1369029\n"
     ]
    }
   ],
   "source": [
    "# inspecting each feature we can see that they are of mixed shapes\n",
    "for x in abmelt_data[\"abituzumab\"]:\n",
    "    print(\"Feature\", x, \"shape =\", abmelt_data[\"abituzumab\"][x].shape)\n",
    "\n",
    "# so each of these features is a grouping of related subfeatures \n",
    "# into data frames where each column is a subfeature\n",
    "cols_count = 0\n",
    "total_data_points = 0\n",
    "for x in abmelt_data[\"abituzumab\"]:\n",
    "    print(x, abmelt_data[\"abituzumab\"][x].shape)\n",
    "    cols_count += abmelt_data[\"abituzumab\"][x].shape[1]\n",
    "    total_data_points += abmelt_data[\"abituzumab\"][x].shape[0] * abmelt_data[\"abituzumab\"][x].shape[1]\n",
    "print(\"Total columns for abituzumab per temp:\", int(cols_count/3))\n",
    "print(\"Total datapoints for abituzumab:\", int(total_data_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "800a87c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bonds_all_300K</th>\n",
       "      <th>bonds_lh_300K</th>\n",
       "      <th>bonds_all_350K</th>\n",
       "      <th>bonds_lh_350K</th>\n",
       "      <th>bonds_all_400K</th>\n",
       "      <th>bonds_lh_400K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>164.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>160.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>165.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>170.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>165.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>160.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>167.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>169.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8001 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      bonds_all_300K  bonds_lh_300K  bonds_all_350K  bonds_lh_350K  \\\n",
       "0              164.0            2.0           157.0            3.0   \n",
       "1              160.0            1.0           162.0            3.0   \n",
       "2              160.0            3.0           166.0            4.0   \n",
       "3              156.0            3.0           168.0            5.0   \n",
       "4              165.0            2.0           168.0            4.0   \n",
       "...              ...            ...             ...            ...   \n",
       "7996           170.0            3.0           157.0            5.0   \n",
       "7997           165.0            4.0           155.0            4.0   \n",
       "7998           160.0            4.0           145.0            4.0   \n",
       "7999           167.0            4.0           154.0            3.0   \n",
       "8000           169.0            4.0           156.0            4.0   \n",
       "\n",
       "      bonds_all_400K  bonds_lh_400K  \n",
       "0              157.0            5.0  \n",
       "1              157.0            5.0  \n",
       "2              162.0            3.0  \n",
       "3              161.0            5.0  \n",
       "4              140.0            2.0  \n",
       "...              ...            ...  \n",
       "7996           133.0            5.0  \n",
       "7997           159.0            6.0  \n",
       "7998           155.0            7.0  \n",
       "7999           153.0            5.0  \n",
       "8000           141.0            4.0  \n",
       "\n",
       "[8001 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look specifically at the bonds feature\n",
    "abmelt_data[\"abituzumab\"][\"bonds\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac37e56d",
   "metadata": {},
   "source": [
    "For the \"bonds\" feature we have 6 columns (subfeatures). There is a bonds_all and bonds_lh for each of three temperatures, 300K, 350K and 400K. These are the temperatures at which the antibody was simulated. the bonds feature itself refers to hydrogen bond counts for the entire Fv region (bonds_all) and just between the heavy and light chain (bonds_lh). These hydrogen bond counts are time series taken over the length of the MD simulation trajectory at snapshots every 0.1ns. Given that simulations were performed for 100ns we would expect 1000 rows in this table however for this and all the other time series data the first 20ns was discard to remove potentially noisy values from equilbiration. This is standard practice in MD.\n",
    "\n",
    "Every major feature in our dataset that has 8001 rows is time series data with equilibration cutoff. Notice how for every feature set the number of columns is divisable by 3, every feature includes a seperate column of values for each temperature. For a more detailed overview of every unique feature/subfeature in this data checkout the featur_reference file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12186a17",
   "metadata": {},
   "source": [
    "Earlier we showed that we have over a million data points per antibody, that's a lot of a data so it would be fruitful to start with some summary statistics and perhaps use them going forward for any future machine learning. In the AbMelt paper they simply use the mean and standard deviation of each subfeature for their models but you should consider other approaches to dataset reduction capable of preserving more information. Much of the skill in machine learning is learning how to approach such large datasets and how the data can be represented for tradeoffs in information capacity, computational efficiency and model appriopriateness. For example, larger models like transformers may benefit from keeping as much information as possible (with one element of data analysis being determining the redundancy of the data for large model learning, how much do we need?). My advice would be to start basic with linear models and multivariate analysis across antibodies on summarisations of the per-feature data or explore information capacity and redundancy with dimensionality reduction techniques such as PCA and clustering. Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hogroast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
